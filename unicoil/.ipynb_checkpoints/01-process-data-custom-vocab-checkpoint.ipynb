{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.033950090408325195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 266662222,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c583b634df9340239862010eabe172fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/267M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "marco_passage = load_dataset(\"Tevatron/msmarco-passage-corpus\")['train']\n",
    "print(len(marco_passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindata = load_dataset(\"Tevatron/msmarco-passage\")['train']\n",
    "\n",
    "# train_metadata = []\n",
    "\n",
    "# for x in tqdm(traindata):\n",
    "    \n",
    "#     if len(x['positive_passages']) > 0:\n",
    "    \n",
    "#         train_metadata.append(\n",
    "#             {\n",
    "#                 \"qid\": x['query_id'],\n",
    "#                 \"query\": x['query'],\n",
    "#                 \"positive_passages\": [d['docid'] for d in x['positive_passages']],\n",
    "#                 \"negative_passages\": [d['docid'] for d in x['negative_passages']],\n",
    "#             }\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distillation with ensemble as an alternative train_metadata\n",
    "\n",
    "# # hard_neg_path = \"/root/splade/data/msmarco/hard_negatives_scores/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz\"\n",
    "# # qrels_path = \"/root/splade/data/msmarco/train_queries/qrels.json\"\n",
    "\n",
    "# hard_neg_path = \"/home/ec2-user/splade_data/msmarco/hard_negatives_scores/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz\"\n",
    "# qrels_path = \"/home/ec2-user/splade_data/msmarco/train_queries/qrels.json\"\n",
    "\n",
    "\n",
    "# with gzip.open(hard_neg_path, \"rb\") as fin:\n",
    "#     scores_dict = pickle.load(fin)\n",
    "    \n",
    "# query_list = list(scores_dict.keys())\n",
    "# with open(qrels_path) as reader:\n",
    "#     qrels = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distill_meta = []\n",
    "\n",
    "# for x in tqdm(train_metadata):\n",
    "#     qid = x['qid']\n",
    "#     query = x['query']\n",
    "#     pos_ids = x['positive_passages']\n",
    "#     pos_scores = [scores_dict[int(qid)][int(docid)] for docid in pos_ids]\n",
    "    \n",
    "#     neg_doc_score = scores_dict[int(qid)]\n",
    "#     neg_doc_score = {str(k): v for k, v in neg_doc_score.items() if str(k) not in pos_ids}\n",
    "#     # top_neg_doc_score = {k: neg_doc_score[k] for k in sorted(neg_doc_score, key=neg_doc_score.get, reverse=True)[:20]}\n",
    "#     top_neg_doc_score = {k: neg_doc_score[k] for k in sorted(neg_doc_score, key=neg_doc_score.get, reverse=True)}\n",
    "#     neg_ids = list(top_neg_doc_score.keys())\n",
    "#     neg_scores = list(top_neg_doc_score.values())\n",
    "    \n",
    "#     distill_meta.append(\n",
    "#         {\n",
    "#             \"qid\": qid,\n",
    "#             \"query\": query,\n",
    "#             \"positive_passages\": pos_ids,\n",
    "#             \"negative_passages\": neg_ids,\n",
    "#             \"positive_teacher_scores\": pos_scores,\n",
    "#             \"negative_teacher_scores\": neg_scores\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2title, id2text = {}, {}\n",
    "\n",
    "for x in tqdm(marco_passage):\n",
    "    \n",
    "    docid = x['docid']\n",
    "    title = x['title']\n",
    "    text = x['text']\n",
    "    \n",
    "    id2title[docid] = title\n",
    "    id2text[docid] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dct(dct, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(dct, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_dct(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 下面这些跑一次就行了\n",
    "\n",
    "# for model_name in [\"bert30k-marcov2-ftv1\", \"bert100k-marcov2-ftv1\", \"bert300k-marcov2-ftv1\"]:\n",
    "# # for model_name in [\"bert36k\", \"wp36k\"]:\n",
    "\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(f\"../custom_vocab/new_trained_mlm/{model_name}\", use_fast=True)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(f\"/home/ec2-user/recovered_repos/texttron-main/custom_vocab/checkpoints/{model_name}\", use_fast=True)\n",
    "#     print(model_name, len(tokenizer.vocab))\n",
    "    \n",
    "#     os.makedirs(f\"tokenized/{model_name}\", exist_ok=True)\n",
    "    \n",
    "#     tokenized_id2title = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2title.items())}\n",
    "#     save_dct(tokenized_id2title, f\"tokenized/{model_name}/id2title.pkl\")\n",
    "#     tokenized_id2text = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2text.items())}\n",
    "#     save_dct(tokenized_id2text, f\"tokenized/{model_name}/id2text.pkl\")\n",
    "    \n",
    "#     os.makedirs(f\"../../data/uniCOIL_{model_name}/training\", exist_ok=True)\n",
    "#     os.makedirs(f\"../../data/uniCOIL_{model_name}/corpus\", exist_ok=True)\n",
    "    \n",
    "#     with open(f'../../data/uniCOIL_{model_name}/training/data.jsonl', 'w') as outfile:\n",
    "\n",
    "#         for m in tqdm(train_metadata):\n",
    "\n",
    "#             tmp_dct = {}\n",
    "#             tmp_dct[\"query_id\"] = m[\"qid\"]\n",
    "#             tmp_dct[\"query\"] = m[\"query\"]\n",
    "#             tmp_dct['positive_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": tokenized_id2text[d]} for d in m['positive_passages']]\n",
    "#             tmp_dct['negative_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": tokenized_id2text[d]} for d in m['negative_passages']]\n",
    "\n",
    "#             json.dump(tmp_dct, outfile)\n",
    "#             outfile.write('\\n')\n",
    "\n",
    "#     with open(f'../../data/uniCOIL_{model_name}/corpus/data.jsonl', 'w') as outfile:\n",
    "\n",
    "#         for docid, text in tqdm(tokenized_id2text.items()):\n",
    "\n",
    "#             tmp_dct = {\n",
    "#                 \"docid\": docid,\n",
    "#                 \"title\": tokenized_id2title[docid],\n",
    "#                 \"text\": text\n",
    "#             }\n",
    "\n",
    "#             json.dump(tmp_dct, outfile)\n",
    "#             outfile.write('\\n')\n",
    "\n",
    "# 这里生成的训练数据是没有tilde expansion的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# os.makedirs(f\"tokenized/{model_name}\", exist_ok=True)\n",
    "    \n",
    "# tokenized_id2title = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2title.items())}\n",
    "# save_dct(tokenized_id2title, f\"tokenized/{model_name}/id2title.pkl\")\n",
    "# tokenized_id2text = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2text.items())}\n",
    "# save_dct(tokenized_id2text, f\"tokenized/{model_name}/id2text.pkl\")\n",
    "\n",
    "# os.makedirs(f\"../../data/uniCOIL_{model_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"../../data/uniCOIL_{model_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# with open(f'../../data/uniCOIL_{model_name}/training/data.jsonl', 'w') as outfile:\n",
    "\n",
    "#     for m in tqdm(train_metadata):\n",
    "\n",
    "#         tmp_dct = {}\n",
    "#         tmp_dct[\"query_id\"] = m[\"qid\"]\n",
    "#         tmp_dct[\"query\"] = m[\"query\"]\n",
    "#         tmp_dct['positive_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": tokenized_id2text[d]} for d in m['positive_passages']]\n",
    "#         tmp_dct['negative_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": tokenized_id2text[d]} for d in m['negative_passages']]\n",
    "\n",
    "#         json.dump(tmp_dct, outfile)\n",
    "#         outfile.write('\\n')\n",
    "\n",
    "# with open(f'../../data/uniCOIL_{model_name}/corpus/data.jsonl', 'w') as outfile:\n",
    "\n",
    "#     for docid, text in tqdm(tokenized_id2text.items()):\n",
    "\n",
    "#         tmp_dct = {\n",
    "#             \"docid\": docid,\n",
    "#             \"title\": tokenized_id2title[docid],\n",
    "#             \"text\": text\n",
    "#         }\n",
    "\n",
    "#         json.dump(tmp_dct, outfile)\n",
    "#         outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"../custom_vocab/new_trained_mlm/bert30k-marcov2-ftv1\", use_fast=True)\n",
    "# print(len(tokenizer.vocab))\n",
    "\n",
    "# os.makedirs(\"tokenized/bert30k\", exist_ok=True)\n",
    "\n",
    "# tokenized_id2title = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2title.items())}\n",
    "# save_dct(tokenized_id2title, f\"tokenized/bert30k/id2title.pkl\")\n",
    "# tokenized_id2text = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2text.items())}\n",
    "# save_dct(tokenized_id2text, f\"tokenized/bert30k/id2text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "# print(\"bert\", len(tokenizer.vocab))\n",
    "\n",
    "# os.makedirs(\"tokenized/bert-base-uncased\", exist_ok=True)\n",
    "\n",
    "# tokenized_id2title = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2title.items())}\n",
    "# save_dct(tokenized_id2title, f\"tokenized/bert-base-uncased/id2title.pkl\")\n",
    "# tokenized_id2text = {k: tokenizer.tokenize(v, max_length=500, truncation=True) for k, v in tqdm(id2text.items())}\n",
    "# save_dct(tokenized_id2text, f\"tokenized/bert-base-uncased/id2text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_expansions(exp_dir_name):\n",
    "    \n",
    "    expansions = {}\n",
    "\n",
    "    tilde_dir = f\"/home/ec2-user/recovered_repos/tilde-main/data/collection/expanded/{exp_dir_name}/top200\"\n",
    "    \n",
    "    print(f\"Reading inferenced expansion data from {tilde_dir}...\", end=\"\\t\")\n",
    "\n",
    "    for file in os.listdir(tilde_dir):\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            file_path = f\"{tilde_dir}/{file}\"\n",
    "            with open(file_path, 'r') as fin:\n",
    "                for line in fin.readlines():\n",
    "                    data = json.loads(line)\n",
    "                    expansions[data['pid']] = data['psg']\n",
    "    \n",
    "    print(\"Done!\")\n",
    "                    \n",
    "    return expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokenized(tokenizer_name):\n",
    "    print(f\"Reading tokenized corpus data from tokenized/{tokenizer_name} ...\", end=\"\\t\")\n",
    "    tokenized_id2text = read_dct(f\"tokenized/{tokenizer_name}/id2text.pkl\")\n",
    "    tokenized_id2title = read_dct(f\"tokenized/{tokenizer_name}/id2title.pkl\")\n",
    "    print(\"Done!\")\n",
    "    return tokenized_id2text, tokenized_id2title\n",
    "\n",
    "def write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text):\n",
    "\n",
    "    with open(f'{full_folder_name}/training/data.jsonl', 'w') as outfile:\n",
    "\n",
    "        for m in tqdm(train_metadata, desc=\"Writing training data: \"):\n",
    "\n",
    "            tmp_dct = {}\n",
    "            tmp_dct[\"query_id\"] = m[\"qid\"]\n",
    "            tmp_dct[\"query\"] = m[\"query\"]\n",
    "            tmp_dct['positive_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": expanded_id2text[d]} for d in m['positive_passages']]\n",
    "            tmp_dct['negative_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": expanded_id2text[d]} for d in m['negative_passages']]\n",
    "\n",
    "            json.dump(tmp_dct, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    with open(f'{full_folder_name}/corpus/data.jsonl', 'w') as outfile:\n",
    "\n",
    "        for docid, text in tqdm(expanded_id2text.items(), desc=\"Writing corpus data: \"):\n",
    "\n",
    "            tmp_dct = {\n",
    "                \"docid\": docid,\n",
    "                \"title\": tokenized_id2title[docid],\n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "            json.dump(tmp_dct, outfile)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "def write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text):\n",
    "\n",
    "    with open(f'{full_folder_name}/training/distill-data.jsonl', 'w') as outfile:\n",
    "\n",
    "        for m in tqdm(distill_meta, desc=\"Writing distill training data: \"):\n",
    "\n",
    "            tmp_dct = {}\n",
    "            tmp_dct[\"query_id\"] = m[\"qid\"]\n",
    "            tmp_dct[\"query\"] = m[\"query\"]\n",
    "            tmp_dct['positive_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": expanded_id2text[d]} for d in m['positive_passages']]\n",
    "            tmp_dct['negative_passages'] = [{\"docid\": d, \"title\": tokenized_id2title[d], \"text\": expanded_id2text[d]} for d in m['negative_passages']]\n",
    "            tmp_dct[\"positive_teacher_scores\"] = m[\"positive_teacher_scores\"]\n",
    "            tmp_dct[\"negative_teacher_scores\"] = m[\"negative_teacher_scores\"]\n",
    "            json.dump(tmp_dct, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jan 20th\n",
    "\n",
    "# outpur_data_folder_name = \"tilde-bert100k-v2v1-borda10\"\n",
    "# full_folder_name = f\"../../data/{outpur_data_folder_name}\"\n",
    "\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k-marcov2-ftv1\")\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert100k-v2v1-borda10\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "# write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jan 21st\n",
    "\n",
    "# outpur_data_folder_name = \"tilde-bert30k-v2v1-borda10\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert30k-marcov2-ftv1\")\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert30k-v2v1-borda10\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "# write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feb.2nd\n",
    "\n",
    "settings = [\n",
    "    # {\n",
    "    #     \"tokenizer_name\": \"bert\",\n",
    "    #     \"output_data_name\": \"tilde-bert-v1\",\n",
    "    #     \"expansion_name\": \"bert-v1-tilde\"\n",
    "    # },\n",
    "\n",
    "    {\n",
    "        \"tokenizer_name\": \"bert\",\n",
    "        \"output_data_name\": \"tilde-bfs-v1\",\n",
    "        \"expansion_name\": \"bfs-v1-tilde\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"tokenizer_name\": \"bert30k-marcov2-ftv1\",\n",
    "        \"output_data_name\": \"tilde-bert30k-v1\",\n",
    "        \"expansion_name\": \"bert30k-v1-tilde\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"tokenizer_name\": \"bert100k-marcov2-ftv1\",\n",
    "        \"output_data_name\": \"tilde-bert100k-v1\",\n",
    "        \"expansion_name\": \"bert100k-v1-tilde\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"tokenizer_name\": \"bert300k-marcov2-ftv1\",\n",
    "        \"output_data_name\": \"tilde-bert300k-v1\",\n",
    "        \"expansion_name\": \"bert300k-v1-tilde\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"tokenizer_name\": \"bert300k-marcov2-ftv1\",\n",
    "        \"output_data_name\": \"tilde-bert100k-v2v1-borda10\",\n",
    "        \"expansion_name\": \"bert300k-v2v1-borda10\"\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feb.2nd\n",
    "\n",
    "for x in settings:\n",
    "\n",
    "    print(x)\n",
    "\n",
    "    output_data_folder_name = x['output_data_name']\n",
    "    full_folder_name = f\"../../data/{output_data_folder_name}\"\n",
    "\n",
    "    os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "    os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "    # read tokenized corpus\n",
    "    tokenized_id2text, tokenized_id2title = read_tokenized(x['tokenizer_name'])\n",
    "\n",
    "    # load expansion\n",
    "    expansions = read_expansions(x['expansion_name'])\n",
    "\n",
    "    # merge expansions into text\n",
    "    expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "    # write data! \n",
    "    write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "    # write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # august 13th\n",
    "\n",
    "outpur_data_folder_name = \"bert30k-v2v1-borda10\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert30k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert30k-v2v1-borda10\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # august 8th\n",
    "\n",
    "outpur_data_folder_name = \"bert100k-v2v1-tilde\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-v2-3epochs-v1-10epochs\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert100k-v2v1-tilde-borda5\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-v2v1-borda5\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert100k-v2v1-tilde-borda10\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-v2v1-borda10\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# august 4th\n",
    "\n",
    "outpur_data_folder_name = \"bert-tilde-cls\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert-base-uncased\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"tilde-cls\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)\n",
    "write_distill_training_data(full_folder_name, distill_meta, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert100k-v2v1-tilde\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-v2-3epochs-v1-10epochs\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert-prf-safe\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert-base-uncased\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert-prf-top10-safe\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert100k-prf-safe\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-prf-epoch20-safe\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bertfromscratch-epoch20-tilde\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert-base-uncased\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bertfromscratch-epoch20\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## create data for bert-tilde-cls, with expansion\n",
    "\n",
    "# outpur_data_folder_name = \"bert-tilde-cls\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert-base-uncased\")\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"tilde-cls\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpur_data_folder_name = \"bert50k-tilde\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert50k\")\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert50k\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpur_data_folder_name = \"bert300k-tilde\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert300k\")\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert300k\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read tokenized corpus\n",
    "# tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpur_data_folder_name = \"bert100k-epoch3\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert100k\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpur_data_folder_name = \"bert100k-epoch15\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert100k-epoch15\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpur_data_folder_name = \"bert100k-epoch20\"\n",
    "# full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "# os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "# os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# # load expansion\n",
    "# expansions = read_expansions(\"bert100k-epoch20\")\n",
    "\n",
    "# # merge expansions into text\n",
    "# expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# # write data! \n",
    "# write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert-tilde-prf\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert-base-uncased\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert-prf-top10\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpur_data_folder_name = \"bert100k-tilde-prf\"\n",
    "full_folder_name = f\"../data/{outpur_data_folder_name}\"\n",
    "\n",
    "os.makedirs(f\"{full_folder_name}/training\", exist_ok=True)\n",
    "os.makedirs(f\"{full_folder_name}/corpus\", exist_ok=True)\n",
    "\n",
    "# read tokenized corpus\n",
    "tokenized_id2text, tokenized_id2title = read_tokenized(\"bert100k\")\n",
    "\n",
    "# load expansion\n",
    "expansions = read_expansions(\"bert100k-prf-top10\")\n",
    "\n",
    "# merge expansions into text\n",
    "expanded_id2text = {k: v + ['[SEP]'] + expansions[k] for k, v in tokenized_id2text.items()}\n",
    "\n",
    "# write data! \n",
    "write_data(full_folder_name, train_metadata, tokenized_id2title, expanded_id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "25b286f208e6a8d9b589b581805d1097be43d3299c4e4a2acc53205c43cdc6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
